<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8"> 
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Masato Hagiwara" />
        <meta name="copyright" content="Masato Hagiwara" />

<meta name="keywords" content="Machine Translation, Language Model, NLProc Cookbook, " />
        <title>Training an N-gram Language Model and Estimating Sentence Probability  · Masato Hagiwara's Page
</title>
        <link href="http://cdn-images.mailchimp.com/embedcode/slim-081711.css" rel="stylesheet" type="text/css">
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/css/bootstrap-combined.min.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/theme/css/style.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/solarizedlight.css" media="screen">
        <link rel="shortcut icon" href="/theme/images/favicon.ico" type="image/x-icon" />
        <link rel="apple-touch-icon" href="/theme/images/apple-touch-icon.png" />
        <link rel="apple-touch-icon" sizes="57x57" href="/theme/images/apple-touch-icon-57x57.png" />
        <link rel="apple-touch-icon" sizes="72x72" href="/theme/images/apple-touch-icon-72x72.png" />
        <link rel="apple-touch-icon" sizes="114x114" href="/theme/images/apple-touch-icon-114x114.png" />
        <link rel="apple-touch-icon" sizes="144x144" href="/theme/images/apple-touch-icon-144x144.png" />
        <link rel="icon" href="/theme/images/apple-touch-icon-144x144.png" />
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-175204-11']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="/"><span class=site-name>Masato Hagiwara's Page</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="">Home</a></li>
                            <li ><a href="/categories.html">Categories</a></li>
                            <li ><a href="/tags.html">Tags</a></li>
                            <li ><a href="/archives.html">Archives</a></li>
                            <li><form class="navbar-search" action="/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
<div class="row-fluid">
    <header class="page_header span10 offset2">
    <h1><a href="/training-an-n-gram-language-model-and-estimating-sentence-probability.html"> Training an N-gram Language Model and Estimating Sentence Probability  </a></h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">

            <h2>Problem</h2>
<p>A (statistical) language model is a model which assigns a probability to a sentence, which is an arbitrary sequence of words. In other words, a language model determines how likely the sentence is in that language. By far the most widely used language model is the n-gram language model, which breaks up a sentence into smaller sequences of words (n-grams) and computes the probability based on individual n-gram probabilities. Given a large corpus of plain text, we would like to train an n-gram language model, and estimate the probability for an arbitrary sentence.</p>
<h2>Solution</h2>
<p>First, we need to prepare a plain text corpus from which we train a language model. We use the sample corpus from <a href="http://corpus.byu.edu/coca/">COCA (Corpus of Contemporary American English)</a>, which can be downloaded from <a href="http://corpus.byu.edu/full-text/samples.asp">here</a>. After downloading 'Word: linear text' → 'COCA: 1.7m' and unzipping the archive, we can clean all the uncompressed text files (<code>w_acad_1990.txt</code>, <code>w_acad_1991.txt</code>, ..., <code>w_spok_2012.txt</code>) using a <a href="https://github.com/mhagiwara/nlproc-cookbook/blob/master/preprocessors/coca/clean.py">cleaning script</a> as follows (we assume the COCA text is unzipped under <code>text/</code> and this is run from the root directory of the Git repository):</p>
<div class="highlight"><pre>cat text/*.txt | python coca/clean.py &gt; text/coca_fulltext.clean.txt
</pre></div>


<p>We use <a href="https://kheafield.com/code/kenlm/">KenLM Language Model Toolkit</a> to build an n-gram language model. KenLM is bundled with the latest version of <a href="http://www.statmt.org/moses/">Moses machine translation system</a>. We'll cover how to install Moses in a separate article. Let's say Moses is installed under <code>mosesdecoder</code> directory. Then we can train a trigram language model using the following command:</p>
<div class="highlight"><pre>mosesdecoder/bin/lmplz -o 3 &lt; text/coca_fulltext.clean.txt &gt; text/coca_fulltext.clean.lm.arpa
</pre></div>


<p>This will create a file in the <em>ARPA format</em> for N-gram back-off models. You can compute the language model probability for any sentences by using the <code>query</code> command:</p>
<div class="highlight"><pre>echo &quot;I am a boy .&quot; | mosesdecoder/bin/query text/coca_fulltext.clean.lm.arpa
</pre></div>


<p>which will output the result as follows (along with other information such as perplexity and time taken to analyze the input):</p>
<div class="highlight"><pre>I=486 2 -1.7037368  am=4760 3 -1.4910358    a=27 3 -1.1888235   boy=10140 2 -3.2120245  .=29 3 -0.6548149   &lt;/s&gt;=2 2 -1.335156  Total: -9.585592 OOV: 0
</pre></div>


<p>The final number <code>-9.585592</code> is the <em>log</em> probability of the sentence. Since it's the logarithm, you need to compute the 10 to the power of that number, which is around 2.60 x 10<sup>-10</sup>.</p>
<h2>Discussion</h2>
<p>KenLM uses a smoothing method called modified Kneser-Ney. Smoothing is a technique to adjust the probability distribution over n-grams to make better estimates of sentence probabilities. For example, any n-grams in a querying sentence which did not appear in the training corpus would be assigned a probability zero, but this is obviously wrong. We cannot cover all the possible n-grams which could appear in a language no matter how large the corpus is, and just because the n-gram didn't appear in a corpus doesn't mean it would <em>never</em> appear in any text. We are not going into the details of smoothing methods in this article. You can find <a href="http://www.foldl.me/2014/kneser-ney-smoothing/">some good introductory articles</a> on Kneaser-Ney smoothing. KenLM is a very memory and time efficient implementation of Kneaser-Ney smoothing and officially distributed with Moses. You can find <a href="http://kheafield.com/code/kenlm/benchmark/">a benchmark article</a> on its performance.</p>
<p>The file created by the <code>lmplz</code> program is in a format called <code>ARPA format</code> for N-gram back-off models. <a href="http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html">This page</a> explains the format in details, but it basically contains log probabilities and back-off weights of each n-gram. In order to compute the probability for a sentence, we look at each n-gram in the sentence from the beginning. If the n-gram is found in the table, we simply read off the log probability and add it (since it's the logarithm, we can use addition instead of product of individual probabilities). If the n-gram is not found in the table, we <em>back off</em> to its lower order n-gram, and use its probability instead, adding the back-off weights (again, we can add them since we are working in the logarithm land).</p>
<p>For example, suppose an excerpt of the ARPA language model file looks like the following:</p>
<div class="highlight"><pre>2-grams
-1.7037368      <span class="nt">&lt;s&gt;</span> I   -0.35425213
-3.1241505      a boy   -0.19261438
-1.9892355      am a    -0.08787394
-1.0562452      boy .   -0.19261438

3-grams
-1.4910358      <span class="nt">&lt;s&gt;</span> I am
-1.1888235      I am a
-0.6548149      a boy .
-1.1425415      . <span class="nt">&lt;/s&gt;</span>  0
</pre></div>


<p>when we are looking at the trigram 'I am a' in the sentence, we can directly read off its log probability <code>-1.1888235</code> (which corresponds to log P('a' | 'I' 'am')) in the table since we do find it in the file. However, the trigram 'am a boy' is not in the table and we need to back-off to 'a boy' (notice we dropped one word from the context, i.e., the preceding words) and use its log probability <code>-3.1241505</code>. Since we backed off, we need to add the back-off weight for 'am a', which is <code>-0.08787394</code>. The sum of these two numbers is the number we saw in the analysis output next to the word 'boy' (<code>-3.2120245</code>). You can also find some explanation of the ARPA format on <a href="http://cmusphinx.sourceforge.net/wiki/sphinx4:standardgrammarformats">the CMU Sphinx page</a>.</p>
            <aside>
            <hr/>
            <nav>
            <ul class="articles_timeline">
 
                <li class="previous_article">« <a href="/using-giza-to-obtain-word-alignment-between-bilingual-sentences.html" title="Previous: Using GIZA++ to Obtain Word Alignment Between Bilingual Sentences">Using GIZA++ to Obtain Word Alignment Between Bilingual Sentences</a></li>
 
                <li class="next_article"><a href="/building-a-statistical-machine-translation-system-using-moses.html" title="Next: Building a Statistical Machine Translation System using Moses">Building a Statistical Machine Translation System using Moses</a> »</li>
            </ul>
            </nav>
            </aside>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
 
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2015-11-25T12:00:00-05:00">Nov 25, 2015</time>
            <h4>Category</h4>
            <a class="category-link" href="/categories.html#NLProc-Cookbook-ref">NLProc Cookbook</a> 
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article"> 
                <li><a href="/tags.html#Language-Model-ref">Language Model
                    <span>1</span>
</a></li>
                <li><a href="/tags.html#Machine-Translation-ref">Machine Translation
                    <span>3</span>
</a></li>
            </ul>

        </div>
        </section>
</div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>            <script src="http://code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.1/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    </body>
</html>